{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88f5ebd",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-11-18T19:31:51.193455Z",
     "iopub.status.busy": "2024-11-18T19:31:51.19306Z",
     "iopub.status.idle": "2024-11-18T19:32:17.320087Z",
     "shell.execute_reply": "2024-11-18T19:32:17.319153Z",
     "shell.execute_reply.started": "2024-11-18T19:31:51.193421Z"
    },
    "papermill": {
     "duration": 0.007724,
     "end_time": "2025-01-08T09:25:31.801964",
     "exception": false,
     "start_time": "2025-01-08T09:25:31.794240",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "source": [
    "This Notebook is adapted from one of the best notebooks of the previous competition by [@emiz6413](https://www.kaggle.com/emiz6413)\n",
    "\n",
    "Original Notebook can be found [here](https://www.kaggle.com/emiz6413)\n",
    "\n",
    "Major changes : \n",
    "* Previous competition used prompts in **English** only. This competition has **multilingual prompts**. I didn't add any translation mechanism here, I think it can be added for better performance.\n",
    "* For the previous competition, we had to submit probabilities for each model,\n",
    " > winner_model_[a/b/tie]\n",
    "\n",
    "In this edition, we just have to submit the winner model ( **model_a or model_b**). I just added a simple function to chose the best model based on the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205a36f",
   "metadata": {
    "papermill": {
     "duration": 0.006761,
     "end_time": "2025-01-08T09:25:31.815864",
     "exception": false,
     "start_time": "2025-01-08T09:25:31.809103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## What this notebook is\n",
    "\n",
    "This is a inference notebook using 4-bit quantized [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) and a LoRA adapter trained using the script uploaded [here](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune).\n",
    "Although we can choose to merge the LoRA adapter to the base model for faster inference, naively doing so could introduce non-negligible quantization error. Therefore, LoRA adapter was kept unmerged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64e0bf90",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-08T09:25:31.831341Z",
     "iopub.status.busy": "2025-01-08T09:25:31.831027Z",
     "iopub.status.idle": "2025-01-08T09:27:26.216792Z",
     "shell.execute_reply": "2025-01-08T09:27:26.215855Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 114.396008,
     "end_time": "2025-01-08T09:27:26.218817",
     "exception": false,
     "start_time": "2025-01-08T09:25:31.822809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/pm-73558185-at-01-08-2025-09-22-49/\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/accelerate-0.32.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/certifi-2024.7.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/filelock-3.15.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/fsspec-2024.6.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/huggingface_hub-0.23.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/idna-3.7-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/jinja2-3.1.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/mpmath-1.3.0-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/networkx-3.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/packaging-24.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/peft-0.11.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/psutil-6.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/requests-2.32.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/sympy-1.12.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/tqdm-4.66.4-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/transformers-4.42.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/typing_extensions-4.12.2-py3-none-any.whl\r\n",
      "Processing /kaggle/input/lmsys-wheel-files/urllib3-2.2.2-py3-none-any.whl\r\n",
      "PyYAML is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "accelerate is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "bitsandbytes is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "charset-normalizer is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "mpmath is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "numpy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "peft is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "requests is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "safetensors is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "sympy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "tokenizers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "tqdm is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "transformers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Installing collected packages: urllib3, typing-extensions, regex, psutil, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, idna, fsspec, filelock, certifi, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, huggingface-hub, torch\r\n",
      "  Attempting uninstall: urllib3\r\n",
      "    Found existing installation: urllib3 1.26.18\r\n",
      "    Uninstalling urllib3-1.26.18:\r\n",
      "      Successfully uninstalled urllib3-1.26.18\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.9.0\r\n",
      "    Uninstalling typing_extensions-4.9.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.9.0\r\n",
      "  Attempting uninstall: regex\r\n",
      "    Found existing installation: regex 2023.12.25\r\n",
      "    Uninstalling regex-2023.12.25:\r\n",
      "      Successfully uninstalled regex-2023.12.25\r\n",
      "  Attempting uninstall: psutil\r\n",
      "    Found existing installation: psutil 5.9.3\r\n",
      "    Uninstalling psutil-5.9.3:\r\n",
      "      Successfully uninstalled psutil-5.9.3\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 21.3\r\n",
      "    Uninstalling packaging-21.3:\r\n",
      "      Successfully uninstalled packaging-21.3\r\n",
      "  Attempting uninstall: networkx\r\n",
      "    Found existing installation: networkx 3.2.1\r\n",
      "    Uninstalling networkx-3.2.1:\r\n",
      "      Successfully uninstalled networkx-3.2.1\r\n",
      "  Attempting uninstall: MarkupSafe\r\n",
      "    Found existing installation: MarkupSafe 2.1.3\r\n",
      "    Uninstalling MarkupSafe-2.1.3:\r\n",
      "      Successfully uninstalled MarkupSafe-2.1.3\r\n",
      "  Attempting uninstall: idna\r\n",
      "    Found existing installation: idna 3.6\r\n",
      "    Uninstalling idna-3.6:\r\n",
      "      Successfully uninstalled idna-3.6\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2024.3.1\r\n",
      "    Uninstalling fsspec-2024.3.1:\r\n",
      "      Successfully uninstalled fsspec-2024.3.1\r\n",
      "  Attempting uninstall: filelock\r\n",
      "    Found existing installation: filelock 3.13.1\r\n",
      "    Uninstalling filelock-3.13.1:\r\n",
      "      Successfully uninstalled filelock-3.13.1\r\n",
      "  Attempting uninstall: certifi\r\n",
      "    Found existing installation: certifi 2024.2.2\r\n",
      "    Uninstalling certifi-2024.2.2:\r\n",
      "      Successfully uninstalled certifi-2024.2.2\r\n",
      "  Attempting uninstall: jinja2\r\n",
      "    Found existing installation: Jinja2 3.1.2\r\n",
      "    Uninstalling Jinja2-3.1.2:\r\n",
      "      Successfully uninstalled Jinja2-3.1.2\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.23.2\r\n",
      "    Uninstalling huggingface-hub-0.23.2:\r\n",
      "      Successfully uninstalled huggingface-hub-0.23.2\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.1.2\r\n",
      "    Uninstalling torch-2.1.2:\r\n",
      "      Successfully uninstalled torch-2.1.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 24.4.1 requires cubinlinker, which is not installed.\r\n",
      "cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 24.4.1 requires ptxcompiler, which is not installed.\r\n",
      "cuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "keras-cv 0.9.0 requires keras-core, which is not installed.\r\n",
      "keras-nlp 0.12.1 requires keras-core, which is not installed.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\r\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\r\n",
      "cudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\r\n",
      "datasets 2.19.2 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2024.6.1 which is incompatible.\r\n",
      "distributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n",
      "gcsfs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2024.6.1 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\r\n",
      "jupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "osmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n",
      "rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\r\n",
      "s3fs 2024.3.1 requires fsspec==2024.3.1, but you have fsspec 2024.6.1 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 certifi-2024.7.4 filelock-3.15.4 fsspec-2024.5.0 huggingface-hub-0.23.4 idna-3.7 jinja2-3.1.4 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 packaging-24.1 psutil-5.9.7 regex-2024.5.15 torch-2.3.1 triton-2.3.1 typing-extensions-4.12.2 urllib3-2.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/lmsys-wheel-files/*.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eea39ea5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:27:26.239622Z",
     "iopub.status.busy": "2025-01-08T09:27:26.239310Z",
     "iopub.status.idle": "2025-01-08T09:27:44.154358Z",
     "shell.execute_reply": "2025-01-08T09:27:44.153662Z"
    },
    "papermill": {
     "duration": 17.927879,
     "end_time": "2025-01-08T09:27:44.156627",
     "exception": false,
     "start_time": "2025-01-08T09:27:26.228748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 09:27:33.067017: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-08 09:27:33.067139: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-08 09:27:33.259844: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc139a6e",
   "metadata": {
    "papermill": {
     "duration": 0.011463,
     "end_time": "2025-01-08T09:27:44.179948",
     "exception": false,
     "start_time": "2025-01-08T09:27:44.168485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8d94a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:27:44.204401Z",
     "iopub.status.busy": "2025-01-08T09:27:44.203786Z",
     "iopub.status.idle": "2025-01-08T09:27:44.210796Z",
     "shell.execute_reply": "2025-01-08T09:27:44.209959Z"
    },
    "papermill": {
     "duration": 0.021175,
     "end_time": "2025-01-08T09:27:44.212461",
     "exception": false,
     "start_time": "2025-01-08T09:27:44.191286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = \"output\"\n",
    "    # checkpoint: str = \"unsloth/gemma-2-9b-it-bnb-4bit\"  # 4-bit quantized gemma-2-9b-instruct\n",
    "    checkpoint: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n",
    "    lora_dir = '/kaggle/working/wsdm2025-multilingual-chatbot-arena/lora_param'\n",
    "    max_length: int = 1024\n",
    "    n_splits: int = 5\n",
    "    fold_idx: int = 0\n",
    "    optim_type: str = \"adamw_8bit\"\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 2  # global batch size is 8 \n",
    "    per_device_eval_batch_size: int = 8\n",
    "    n_epochs: int = 1\n",
    "    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    "    lr: float = 2e-4\n",
    "    warmup_steps: int = 20\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: float = lora_r * 2\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_bias: str = \"none\"\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c8f0ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:27:44.233207Z",
     "iopub.status.busy": "2025-01-08T09:27:44.232963Z",
     "iopub.status.idle": "2025-01-08T09:27:44.283724Z",
     "shell.execute_reply": "2025-01-08T09:27:44.283037Z"
    },
    "papermill": {
     "duration": 0.062954,
     "end_time": "2025-01-08T09:27:44.285534",
     "exception": false,
     "start_time": "2025-01-08T09:27:44.222580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    num_train_epochs=config.n_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    optim=config.optim_type,\n",
    "    fp16=True,\n",
    "    learning_rate=config.lr,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_dir=\"./logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4dcb0bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:27:44.306439Z",
     "iopub.status.busy": "2025-01-08T09:27:44.305946Z",
     "iopub.status.idle": "2025-01-08T09:27:44.310819Z",
     "shell.execute_reply": "2025-01-08T09:27:44.309965Z"
    },
    "papermill": {
     "duration": 0.016833,
     "end_time": "2025-01-08T09:27:44.312490",
     "exception": false,
     "start_time": "2025-01-08T09:27:44.295657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    # only target self-attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3576524d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:27:44.332523Z",
     "iopub.status.busy": "2025-01-08T09:27:44.332287Z",
     "iopub.status.idle": "2025-01-08T09:27:45.307575Z",
     "shell.execute_reply": "2025-01-08T09:27:45.306655Z"
    },
    "papermill": {
     "duration": 0.98767,
     "end_time": "2025-01-08T09:27:45.309796",
     "exception": false,
     "start_time": "2025-01-08T09:27:44.322126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)\n",
    "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b8ae29",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-08T09:27:45.331028Z",
     "iopub.status.busy": "2025-01-08T09:27:45.330758Z",
     "iopub.status.idle": "2025-01-08T09:28:35.268627Z",
     "shell.execute_reply": "2025-01-08T09:28:35.267844Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 49.950523,
     "end_time": "2025-01-08T09:28:35.270641",
     "exception": false,
     "start_time": "2025-01-08T09:27:45.320118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abb4db7a6284d239a55800c71f2c751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForSequenceClassification(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2SdpaAttention(\n",
       "              (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm()\n",
       "            (post_attention_layernorm): Gemma2RMSNorm()\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "          )\n",
       "          (16-41): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2SdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm()\n",
       "            (post_attention_layernorm): Gemma2RMSNorm()\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=3584, out_features=3, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=3584, out_features=3, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=3584, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=3584, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    # num_labels=2,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.classifier = torch.nn.Linear(in_features=3584, out_features=2)  # 2クラス分類に変更\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c291ee",
   "metadata": {
    "papermill": {
     "duration": 0.009605,
     "end_time": "2025-01-08T09:28:35.290666",
     "exception": false,
     "start_time": "2025-01-08T09:28:35.281061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load & pre-process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567fc939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:28:35.311557Z",
     "iopub.status.busy": "2025-01-08T09:28:35.311249Z",
     "iopub.status.idle": "2025-01-08T09:28:37.297913Z",
     "shell.execute_reply": "2025-01-08T09:28:37.296914Z"
    },
    "papermill": {
     "duration": 2.000427,
     "end_time": "2025-01-08T09:28:37.301008",
     "exception": false,
     "start_time": "2025-01-08T09:28:35.300581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = \"/kaggle/input/wsdm-cup-multilingual-chatbot-arena\"\n",
    "\n",
    "train = pd.read_parquet(f\"{INPUT_DIR}/train.parquet\")\n",
    "# test = pd.read_parquet(f\"{INPUT_DIR}/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9721a809",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:28:37.326869Z",
     "iopub.status.busy": "2025-01-08T09:28:37.326559Z",
     "iopub.status.idle": "2025-01-08T09:28:40.330745Z",
     "shell.execute_reply": "2025-01-08T09:28:40.329923Z"
    },
    "papermill": {
     "duration": 3.0172,
     "end_time": "2025-01-08T09:28:40.332827",
     "exception": false,
     "start_time": "2025-01-08T09:28:37.315627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(train)\n",
    "ds = ds.select(torch.arange(3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d103174f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:28:40.355811Z",
     "iopub.status.busy": "2025-01-08T09:28:40.355205Z",
     "iopub.status.idle": "2025-01-08T09:28:40.362674Z",
     "shell.execute_reply": "2025-01-08T09:28:40.361908Z"
    },
    "papermill": {
     "duration": 0.020778,
     "end_time": "2025-01-08T09:28:40.364259",
     "exception": false,
     "start_time": "2025-01-08T09:28:40.343481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, max_length: int) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        # Ensure that the keys exist in the batch before processing\n",
    "        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch.get(\"prompt\", [])]\n",
    "        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch.get(\"response_a\", [])]\n",
    "        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch.get(\"response_b\", [])]\n",
    "        \n",
    "        # Concatenate all parts into one text field for tokenization\n",
    "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        \n",
    "        # Tokenize the texts\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True, padding=True)\n",
    "        \n",
    "        # Handle the winner labels (mapping winner from 'model_a' to 0, 'model_b' to 1)\n",
    "        labels = []\n",
    "        winners = batch.get(\"winner\", [])\n",
    "        \n",
    "        for winner in winners:\n",
    "            if winner == 'model_a':\n",
    "                label = 0\n",
    "            elif winner == 'model_b':\n",
    "                label = 1\n",
    "            # If the winner is neither 'model_a' nor 'model_b', you could choose to skip or handle the error here\n",
    "            else:\n",
    "                continue  # Or use `label = None` if you want to handle such cases separately\n",
    "                \n",
    "            labels.append(label)\n",
    "        \n",
    "        # Return tokenized output with labels\n",
    "        return {**tokenized, \"labels\": labels}\n",
    "\n",
    "    @staticmethod\n",
    "    def process_text(text: str) -> str:\n",
    "        return text.replace(\"null\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7d454fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:28:40.385765Z",
     "iopub.status.busy": "2025-01-08T09:28:40.385078Z",
     "iopub.status.idle": "2025-01-08T09:28:45.538052Z",
     "shell.execute_reply": "2025-01-08T09:28:45.537360Z"
    },
    "papermill": {
     "duration": 5.165483,
     "end_time": "2025-01-08T09:28:45.539721",
     "exception": false,
     "start_time": "2025-01-08T09:28:40.374238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c41d3a044a40da8d0bf90d055ff58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)\n",
    "ds = ds.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7baf8b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:28:45.562521Z",
     "iopub.status.busy": "2025-01-08T09:28:45.562221Z",
     "iopub.status.idle": "2025-01-08T09:28:45.566945Z",
     "shell.execute_reply": "2025-01-08T09:28:45.566147Z"
    },
    "papermill": {
     "duration": 0.017576,
     "end_time": "2025-01-08T09:28:45.568620",
     "exception": false,
     "start_time": "2025-01-08T09:28:45.551044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n",
    "    return {\"acc\": acc, \"log_loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "436a201c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:28:45.591205Z",
     "iopub.status.busy": "2025-01-08T09:28:45.590765Z",
     "iopub.status.idle": "2025-01-08T09:28:45.595289Z",
     "shell.execute_reply": "2025-01-08T09:28:45.594523Z"
    },
    "papermill": {
     "duration": 0.01676,
     "end_time": "2025-01-08T09:28:45.596792",
     "exception": false,
     "start_time": "2025-01-08T09:28:45.580032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred: EvalPrediction) -> dict:\n",
    "    # Extract predictions and labels from the EvalPrediction object\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    # Convert logits to predicted labels (assuming binary classification with logits)\n",
    "    pred_labels = logits.argmax(axis=-1)  # For multi-class, use argmax along the correct axis\n",
    "\n",
    "    # Calculate accuracy and other metrics\n",
    "    accuracy = accuracy_score(labels, pred_labels)\n",
    "    # precision = precision_score(labels, pred_labels, average='binary')\n",
    "    # recall = recall_score(labels, pred_labels, average='binary')\n",
    "    # f1 = f1_score(labels, pred_labels, average='binary')\n",
    "\n",
    "    # Return the metrics as a dictionary\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        # \"precision\": precision,\n",
    "        # \"recall\": recall,\n",
    "        # \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8280b5e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:28:45.617377Z",
     "iopub.status.busy": "2025-01-08T09:28:45.617131Z",
     "iopub.status.idle": "2025-01-08T09:28:45.624253Z",
     "shell.execute_reply": "2025-01-08T09:28:45.623453Z"
    },
    "papermill": {
     "duration": 0.019135,
     "end_time": "2025-01-08T09:28:45.625753",
     "exception": false,
     "start_time": "2025-01-08T09:28:45.606618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(config.n_splits)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfe315b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:28:45.647086Z",
     "iopub.status.busy": "2025-01-08T09:28:45.646668Z",
     "iopub.status.idle": "2025-01-08T15:40:24.283240Z",
     "shell.execute_reply": "2025-01-08T15:40:24.282484Z"
    },
    "papermill": {
     "duration": 22298.648991,
     "end_time": "2025-01-08T15:40:24.285018",
     "exception": false,
     "start_time": "2025-01-08T09:28:45.636027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 6:11:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.684200</td>\n",
       "      <td>0.702079</td>\n",
       "      <td>0.546667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.9227921708424887, metrics={'train_runtime': 22297.4885, 'train_samples_per_second': 0.108, 'train_steps_per_second': 0.027, 'total_flos': 1.22803984171008e+17, 'train_loss': 0.9227921708424887, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx, eval_idx = folds[config.fold_idx]\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c06abbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:40:24.308457Z",
     "iopub.status.busy": "2025-01-08T15:40:24.307851Z",
     "iopub.status.idle": "2025-01-08T15:40:24.399614Z",
     "shell.execute_reply": "2025-01-08T15:40:24.398699Z"
    },
    "papermill": {
     "duration": 0.104887,
     "end_time": "2025-01-08T15:40:24.401061",
     "exception": false,
     "start_time": "2025-01-08T15:40:24.296174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(config.lora_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f436953",
   "metadata": {
    "papermill": {
     "duration": 0.010204,
     "end_time": "2025-01-08T15:40:24.421685",
     "exception": false,
     "start_time": "2025-01-08T15:40:24.411481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34e7983d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:40:24.443389Z",
     "iopub.status.busy": "2025-01-08T15:40:24.443152Z",
     "iopub.status.idle": "2025-01-08T15:40:24.446539Z",
     "shell.execute_reply": "2025-01-08T15:40:24.445754Z"
    },
    "papermill": {
     "duration": 0.015902,
     "end_time": "2025-01-08T15:40:24.448045",
     "exception": false,
     "start_time": "2025-01-08T15:40:24.432143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def process_text(text):\n",
    "#     return text.replace(\"null\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43a9390e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:40:24.469653Z",
     "iopub.status.busy": "2025-01-08T15:40:24.469399Z",
     "iopub.status.idle": "2025-01-08T15:40:24.472704Z",
     "shell.execute_reply": "2025-01-08T15:40:24.472003Z"
    },
    "papermill": {
     "duration": 0.015731,
     "end_time": "2025-01-08T15:40:24.474104",
     "exception": false,
     "start_time": "2025-01-08T15:40:24.458373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def tokenize_test(row,tokenizer):\n",
    "#     prompt = [\"<prompt>: \" + process_text(t) for t in row[\"prompt\"]]\n",
    "#     response_a = [\"\\n\\n<response_a>: \" + process_text(t) for t in row[\"response_a\"]]\n",
    "#     response_b = [\"\\n\\n<response_b>: \" + process_text(t) for t in row[\"response_b\"]]\n",
    "\n",
    "#     # Concatenate all parts into one text field for tokenization\n",
    "#     texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        \n",
    "#     # Tokenize the texts\n",
    "#     tokenized = tokenizer(texts, max_length=1024, truncation=True, padding=True)\n",
    "\n",
    "#     return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c9abac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:40:24.495920Z",
     "iopub.status.busy": "2025-01-08T15:40:24.495700Z",
     "iopub.status.idle": "2025-01-08T15:40:24.499250Z",
     "shell.execute_reply": "2025-01-08T15:40:24.498411Z"
    },
    "papermill": {
     "duration": 0.016207,
     "end_time": "2025-01-08T15:40:24.500779",
     "exception": false,
     "start_time": "2025-01-08T15:40:24.484572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preds_valid = trainer.predict(ds.select(eval_idx)).predictions\n",
    "\n",
    "# preds_oof[idx_valid] = torch.from_numpy(preds).float().softmax(dim=-1).numpy()[:, -1]\n",
    "\n",
    "# ds_test = Dataset.from_pandas(test)\n",
    "# ds_test = ds_test.map(\n",
    "#     tokenize_test,\n",
    "#     fn_kwargs={'tokenizer': tokenizer},\n",
    "#     batched=True,\n",
    "# )\n",
    "\n",
    "# # メモリ節約のために元のテキスト列を削除（オプション）\n",
    "# ds_test = ds_test.remove_columns(['prompt', 'response_a', 'response_b'])\n",
    "\n",
    "# preds_test = trainer.predict(ds_test).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f9ed546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:40:24.523037Z",
     "iopub.status.busy": "2025-01-08T15:40:24.522822Z",
     "iopub.status.idle": "2025-01-08T15:40:24.526189Z",
     "shell.execute_reply": "2025-01-08T15:40:24.525393Z"
    },
    "papermill": {
     "duration": 0.016174,
     "end_time": "2025-01-08T15:40:24.527823",
     "exception": false,
     "start_time": "2025-01-08T15:40:24.511649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prob = torch.from_numpy(preds_test).float().softmax(dim=-1).numpy()\n",
    "# prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70f8ca7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:40:24.550137Z",
     "iopub.status.busy": "2025-01-08T15:40:24.549903Z",
     "iopub.status.idle": "2025-01-08T15:40:24.553160Z",
     "shell.execute_reply": "2025-01-08T15:40:24.552381Z"
    },
    "papermill": {
     "duration": 0.016082,
     "end_time": "2025-01-08T15:40:24.554788",
     "exception": false,
     "start_time": "2025-01-08T15:40:24.538706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_mapping = {0: 'model_a', 1: 'model_b'} \n",
    "# test[\"winner\"] = np.argmax(prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59083c6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:40:24.576148Z",
     "iopub.status.busy": "2025-01-08T15:40:24.575945Z",
     "iopub.status.idle": "2025-01-08T15:40:24.579166Z",
     "shell.execute_reply": "2025-01-08T15:40:24.578528Z"
    },
    "papermill": {
     "duration": 0.015509,
     "end_time": "2025-01-08T15:40:24.580575",
     "exception": false,
     "start_time": "2025-01-08T15:40:24.565066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test['winner'] = test['winner'].map(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5477881e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:40:24.602006Z",
     "iopub.status.busy": "2025-01-08T15:40:24.601800Z",
     "iopub.status.idle": "2025-01-08T15:40:24.605011Z",
     "shell.execute_reply": "2025-01-08T15:40:24.604233Z"
    },
    "papermill": {
     "duration": 0.015661,
     "end_time": "2025-01-08T15:40:24.606476",
     "exception": false,
     "start_time": "2025-01-08T15:40:24.590815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub=test[[\"id\",\"winner\"]]\n",
    "# sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "542e8ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T15:40:24.627825Z",
     "iopub.status.busy": "2025-01-08T15:40:24.627600Z",
     "iopub.status.idle": "2025-01-08T15:40:24.630817Z",
     "shell.execute_reply": "2025-01-08T15:40:24.630046Z"
    },
    "papermill": {
     "duration": 0.015572,
     "end_time": "2025-01-08T15:40:24.632371",
     "exception": false,
     "start_time": "2025-01-08T15:40:24.616799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10131489,
     "sourceId": 86946,
     "sourceType": "competition"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5369301,
     "sourceId": 8926343,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22498.227287,
   "end_time": "2025-01-08T15:40:27.539672",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-08T09:25:29.312385",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0756689eb1914912b1fc64ff040af4ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0bd7054fce8d4d31baee468de5be0386": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "12738101ad3e4613b1acda9648a71c25": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1a22877efcfe48c89f5b41e3ed2a7f98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_34d7e27f78d54dd3b144c0acbf5ca58c",
       "placeholder": "​",
       "style": "IPY_MODEL_4c4395b247a14f6391f44c63f9464532",
       "value": "Map: 100%"
      }
     },
     "2babe766f4b141a4a30626e11f9d124b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "34d7e27f78d54dd3b144c0acbf5ca58c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "389fbdfcc1074141b06fa6e44dba2163": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4de8c32d4ef345f4aa3f456025b131f9",
       "max": 3000.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3de97c0d763244388f98ec671060d696",
       "value": 3000.0
      }
     },
     "3de97c0d763244388f98ec671060d696": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "412769c482d5431aa8039fc1234f9b1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "492c8ef2d08b479381750f05e8e12112": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4c4395b247a14f6391f44c63f9464532": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4de8c32d4ef345f4aa3f456025b131f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "57620de482fd45f8b58e2ba5bec65162": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5abb4db7a6284d239a55800c71f2c751": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9899785b71d642fbb1828d91c344f2fb",
        "IPY_MODEL_76dad87223394c3d979ae38ccf817ce2",
        "IPY_MODEL_ef25da58aac84563b35f57dd3e285d12"
       ],
       "layout": "IPY_MODEL_492c8ef2d08b479381750f05e8e12112"
      }
     },
     "71c41d3a044a40da8d0bf90d055ff58a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1a22877efcfe48c89f5b41e3ed2a7f98",
        "IPY_MODEL_389fbdfcc1074141b06fa6e44dba2163",
        "IPY_MODEL_d724ad35bc7340bcb2441159bc094b14"
       ],
       "layout": "IPY_MODEL_e7e4d9d5c89248ceb936b5e0bb348971"
      }
     },
     "76dad87223394c3d979ae38ccf817ce2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0bd7054fce8d4d31baee468de5be0386",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_412769c482d5431aa8039fc1234f9b1a",
       "value": 2.0
      }
     },
     "8b7cb3f51f8a417eae2fb5d86afc872d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9899785b71d642fbb1828d91c344f2fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0756689eb1914912b1fc64ff040af4ba",
       "placeholder": "​",
       "style": "IPY_MODEL_c675752817c04029baaf396bfc3e2d21",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "c675752817c04029baaf396bfc3e2d21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d724ad35bc7340bcb2441159bc094b14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_57620de482fd45f8b58e2ba5bec65162",
       "placeholder": "​",
       "style": "IPY_MODEL_8b7cb3f51f8a417eae2fb5d86afc872d",
       "value": " 3000/3000 [00:04&lt;00:00, 633.00 examples/s]"
      }
     },
     "e7e4d9d5c89248ceb936b5e0bb348971": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ef25da58aac84563b35f57dd3e285d12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_12738101ad3e4613b1acda9648a71c25",
       "placeholder": "​",
       "style": "IPY_MODEL_2babe766f4b141a4a30626e11f9d124b",
       "value": " 2/2 [00:48&lt;00:00, 21.65s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
